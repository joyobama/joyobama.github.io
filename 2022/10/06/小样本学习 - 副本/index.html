<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>小样本学习 | Time is Gone</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="小样本学习方法解决问题：学习过程中过程数据不足。 12graph LRA(小样本) --&gt; B(样本多样性低,模型泛化能力差)  基于无标签的数据增强方法半监督学习半监督学习：利用无标签数据对小样本数据集进行扩充的主要方法之一。通俗的来说就是先利用现有的少量的有标签数据训练一个弱模型，然后利用这个弱模型去识别判定无标签样本，再利用训练结果继续训练弱模型，来提高模型的判别能力。 数据合成利用小">
<meta property="og:type" content="article">
<meta property="og:title" content="小样本学习">
<meta property="og:url" content="https://joyobama.github.io/2022/10/06/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%20-%20%E5%89%AF%E6%9C%AC/index.html">
<meta property="og:site_name" content="Time is Gone">
<meta property="og:description" content="小样本学习方法解决问题：学习过程中过程数据不足。 12graph LRA(小样本) --&gt; B(样本多样性低,模型泛化能力差)  基于无标签的数据增强方法半监督学习半监督学习：利用无标签数据对小样本数据集进行扩充的主要方法之一。通俗的来说就是先利用现有的少量的有标签数据训练一个弱模型，然后利用这个弱模型去识别判定无标签样本，再利用训练结果继续训练弱模型，来提高模型的判别能力。 数据合成利用小">
<meta property="og:locale">
<meta property="og:image" content="https://joyobama.github.io/image-20211007203852513.png">
<meta property="og:image" content="https://joyobama.github.io/Desktop/data/%E6%95%85%E9%9A%9C%E6%A0%87%E8%AF%86.png">
<meta property="og:image" content="https://joyobama.github.io/%E6%95%85%E9%9A%9C%E7%AD%89%E7%BA%A7.png">
<meta property="og:image" content="https://joyobama.github.io/gif-16336577977311.latex">
<meta property="og:image" content="https://joyobama.github.io/gif-16336578232435.latex">
<meta property="og:image" content="https://joyobama.github.io/gif-16336578314437.latex">
<meta property="og:image" content="https://joyobama.github.io/gif-163365902171813.latex">
<meta property="og:image" content="https://joyobama.github.io/gif-163365902172115.latex">
<meta property="og:image" content="https://private.codecogs.com/gif.latex?H_%7Bi%7D">
<meta property="og:image" content="https://joyobama.github.io/S_%7Bmaj%7D%7D.gif">
<meta property="og:image" content="https://joyobama.github.io/gif-16336577977311.latex">
<meta property="og:image" content="https://joyobama.github.io/gif-16336578232435.latex">
<meta property="og:image" content="https://joyobama.github.io/gif-16336578314437.latex">
<meta property="og:image" content="https://joyobama.github.io/image-20211008105604570.png">
<meta property="og:image" content="https://joyobama.github.io/image-20211008105626919.png">
<meta property="og:image" content="https://joyobama.github.io/v2-48a6a2a8b213f4bd52dfb694ad292f00_720w.jpg">
<meta property="og:image" content="https://joyobama.github.io/90.jpeg">
<meta property="og:image" content="https://joyobama.github.io/v2-64263acb7eeb012f7fa7e80446d4dac3_r.jpg">
<meta property="og:image" content="https://joyobama.github.io/v2-ce9f2c59b74e1970692fafe7d713fc99_r.jpg">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2020103111192626.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MDE1MDU5,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://joyobama.github.io/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MDE1MDU5,size_16,color_FFFFFF,t_70#pic_center.png">
<meta property="og:image" content="https://joyobama.github.io/1.jpg">
<meta property="article:published_time" content="2022-10-06T13:00:15.677Z">
<meta property="article:modified_time" content="2021-10-08T07:47:23.622Z">
<meta property="article:author" content="Joy_Obama">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://joyobama.github.io/image-20211007203852513.png">
  
    <link rel="alternate" href="/atom.xml" title="Time is Gone" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Time is Gone</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Joy_Obama</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Suche"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://joyobama.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-小样本学习 - 副本" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/10/06/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%20-%20%E5%89%AF%E6%9C%AC/" class="article-date">
  <time class="dt-published" datetime="2022-10-06T13:00:15.677Z" itemprop="datePublished">2022-10-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      小样本学习
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="小样本学习方法"><a href="#小样本学习方法" class="headerlink" title="小样本学习方法"></a>小样本学习方法</h1><p>解决问题：学习过程中过程数据不足。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A(小样本) --&gt; B(样本多样性低,模型泛化能力差)</span><br></pre></td></tr></table></figure>

<h2 id="基于无标签的数据增强方法"><a href="#基于无标签的数据增强方法" class="headerlink" title="基于无标签的数据增强方法"></a>基于无标签的数据增强方法</h2><h3 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h3><p>半监督学习：利用无标签数据对小样本数据集进行扩充的主要方法之一。通俗的来说就是先利用现有的少量的有标签数据训练一个弱模型，然后利用这个弱模型去识别判定无标签样本，再利用训练结果继续训练弱模型，来提高模型的判别能力。</p>
<h3 id="数据合成"><a href="#数据合成" class="headerlink" title="数据合成"></a>数据合成</h3><p>利用小样本数据合成新的带标签数据来扩充训练，不平衡学习，一般采用到的算法是GAN，用于生成类似的带标签样本来平衡数据集。</p>
<h3 id="特征增强"><a href="#特征增强" class="headerlink" title="特征增强"></a>特征增强</h3><p>通过增强样本的特征空间来提高样本的多样性，特征空间的变换主要采用局部或全局的融合。特征增强本质上就是进行特征空间的变换，利用样本的冗余属性，提高样本之间的区分度，从而提高样本的多样性。</p>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ol>
<li>可以合理的利用无标签数据。</li>
<li>可以更好的利用冗余特征，用于特征的增强。</li>
</ol>
<h2 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h2><p>利用旧知识学习新知识，旧知识的样本空间域称之为源域，应用的样本空间域为目标域。迁移学习允许源域与目标域的样本，任务，分布有较大的差异。其有效性基本取决于领域之间的相似程度，同时迁移学习存在负迁移的现象。</p>
<h3 id="归纳迁移学习"><a href="#归纳迁移学习" class="headerlink" title="归纳迁移学习"></a>归纳迁移学习</h3><p><strong>源域与目标域相同，任务不相同</strong>，分为多任务学习(有标签)和自学习(无标签)，例如同一设备，不同工况等情形。具体的，主要是在源域中进行模型的训练，得到一个较为通用的模型，然后再在目标域进行训练微调或者直接进行使用，目标域可以是小样本或者无样本。</p>
<h3 id="直推式迁移学习"><a href="#直推式迁移学习" class="headerlink" title="直推式迁移学习"></a>直推式迁移学习</h3><p><strong>源域(有标)与目标域(无标)有相似之处</strong>，例如电机上的轴承数据进行训练，去识别机车上故障。</p>
<h3 id="无监督迁移学习"><a href="#无监督迁移学习" class="headerlink" title="无监督迁移学习"></a>无监督迁移学习</h3><p>类似于归纳迁移学习，但着重于目标域的无监督学习。</p>
<h3 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h3><ol>
<li>负迁移表示不但没有提高模型的辨识能力，反而降低了识别率、</li>
<li>在非简单分类或回归问题问题时，如何优化算法。</li>
<li>如何跨领域学习</li>
<li>偏数据的处理，面临的问题是，数据收集的时候与下一次时刻的分布不一致。</li>
<li>迁移学习与多种深度学习的方法相结合的图像描述方法，图像呈现方法。</li>
</ol>
<h2 id="元学习"><a href="#元学习" class="headerlink" title="元学习"></a>元学习</h2><p>利用先验知识来指导模型在新任务中更快的学习。</p>
<h3 id="基于度量"><a href="#基于度量" class="headerlink" title="基于度量"></a>基于度量</h3><p>学习一个嵌入网络，将原始样本转换为合适的样本特征空间，来计算样本之间的相似程度。本质上就是样本特征空间的转换。来获得更好的样本区分度，在进行下一步学习时可转换为简单分类或回归问题。</p>
<h3 id="基于优化"><a href="#基于优化" class="headerlink" title="基于优化"></a>基于优化</h3><p>在此方法中，优化初始化表征、优化网络参数方法在故障诊断领域均有应用。优化初始化表征方法关键为保证网络自身学习到一个好的初始化，使模型易于微调，不再需要手动配置模型的初始化权重参数，其中最经典的方法为不可知元模型，它的关键位要使新任务的损失函数对初始化权重敏感度最大化，估计得到参数最优解。</p>
<hr>
<h2 id="工作方向-故障诊断"><a href="#工作方向-故障诊断" class="headerlink" title="工作方向-故障诊断"></a>工作方向-故障诊断</h2><p>根据现有数据进行故障诊断模型的建立，进行故障的诊断。</p>
<p><img src="/image-20211007203852513.png" alt="image-20211007203852513"></p>
<h3 id="数据集特点"><a href="#数据集特点" class="headerlink" title="数据集特点"></a>数据集特点</h3><p> <img src="/../../../Desktop/data/%E6%95%85%E9%9A%9C%E6%A0%87%E8%AF%86.png" alt="故障标识"></p>
<p>故障标识</p>
<p><img src="/%E6%95%85%E9%9A%9C%E7%AD%89%E7%BA%A7.png" alt="故障等级"></p>
<p>故障等级</p>
<ol>
<li>除了车辆6外，其他车辆故障样本都极度<strong>不平衡</strong>。</li>
<li>故障模型建立后，一般情况下不会再应用于现有的数据集，而是<strong>该辆车以后所采集的数据或其他车辆的故障诊断</strong>。</li>
<li>数据各项属性是<strong>异质</strong>的，且具有一定的冗余。</li>
</ol>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>针对数据集不平衡的情况，属于不平衡学习范畴。</p>
<h3 id="不平衡学习-Imbalanced-Learning"><a href="#不平衡学习-Imbalanced-Learning" class="headerlink" title="不平衡学习(Imbalanced Learning)"></a>不平衡学习(Imbalanced Learning)</h3><p><strong>定义：</strong>在数据集中，有一类含有的数据要远远多于其他类的数据（类别分布不平衡），例如负例和正例的比例为100:1。具体分为类间不平衡和类内不平衡。</p>
<p>解决不平衡问题主要的方法：</p>
<ol>
<li>随机采样</li>
<li>代价敏感学习</li>
<li>数据生成(以GAN为代表)</li>
</ol>
<h4 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h4><h5 id="过采样"><a href="#过采样" class="headerlink" title="过采样"></a>过采样</h5><p>从少数类集合中随机选择一个子集，如果要增强数据集，那么只需要在这个少数类中重复随机采样，把这些样本集合增加到原始数据集中。处理之后的数据集中存在很多重复的样本，可能会导致学习的规则比较具体，因此，可能会导致过拟合。</p>
<h5 id="欠采样"><a href="#欠采样" class="headerlink" title="欠采样"></a>欠采样</h5><p>在数据集中的多数类进行随机采样，然后把随机得到的样本集合从数据集中移除。当我们移除一些数据的时候，可能会失去一些重要的信息。为了避免这种信息损失带来的影响，在欠采样中，主要有两个方法：<strong>easyEnsemble</strong>，<strong>BalanceCascade</strong>。</p>
<p><strong>EasyEnsemble:</strong></p>
<p>EasyEnsemble主要是在数据集的多数类中进行随机采样，使得$|E|&#x3D;|S_{min}|$，然后从原始数据集中将这些样本去掉。这种方法利用到集成学习的方法，具体过程如下：</p>
<ol>
<li><p>初始化：<em>i</em>表示迭代次数，T表示最大迭代次数，表示每一次训练得到的分类器，表示中的弱分类器的数量；</p>
</li>
<li><p>$i&#x3D;0$；</p>
</li>
<li><p>从多数类<img src="/gif-16336577977311.latex" alt="img">中随机选择一个子集$E$,$|E|&#x3D;|S_{min}|$。</p>
</li>
<li><p>使用子集<img src="/gif-16336578232435.latex" alt="img">和<img src="/gif-16336578314437.latex" alt="img">，训练分类器$H_i$，$H_i$是由$s_i$个弱分类器$h_{i,j}$集合而成，其中每个弱分类器的权重为$\alpha_{i,j}$，阈值为：$\theta_i$。因此<br>$$<br>H_i(x)&#x3D;sgn(\sum_{j&#x3D;1}^{s_i}\alpha_{i,j}h_{i,j}(x)-\theta_i)<br>$$</p>
</li>
<li><p>重复步骤 3-5，直到$i&#x3D;T$；</p>
</li>
<li><p>最后，输出一个集成学习器<br>$$<br>H(x)&#x3D;sgn(\sum_{i&#x3D;1}^{T}\sum_{j&#x3D;1}^{s_i}\alpha_{i,j}h_{i,j}(x)-\sum_{i&#x3D;1}^{T}\theta_i)<br>$$</p>
</li>
</ol>
<p>EasyEnsemble产生T个子问题，每一个子问题都会生成一个集成的学习器，这个学习器是由个弱分类器集成的。每一步中，弱分类的训练采用Bootstrap采样方法。我们可以将每个弱分类器看成是从不同的训练集中得到的特征，每个集成的分类器看成是不同的特征的线性组合。最后把这些T个集成的分类器再进行集成。因此，我们可以把EasyEnsemble方法看成是集成的集成，它是一个无监督的方法。</p>
<p><strong>BalanceCascade:</strong></p>
<p>BalanceCascade方法类似于EasyEnsemble，不同之处在于：</p>
<ul>
<li>BalanceCascade是一个监督学习的方法；</li>
<li>BalanceCascade每一次生成一个$H_i$，对于样本$x_i\in S_{maj}$，如果该样本可以被正确分类，那么说明该样本是冗余的，因此，从多数类中把该样本删除；</li>
</ul>
<p>BalanceCascade的主要过程：</p>
<ol>
<li><p>初始化：i表示迭代次数，T表示最大迭代次数，<img src="/gif-163365902171813.latex" alt="H_{i}">表示每一次训练得到的分类器，<img src="/gif-163365902172115.latex" alt="s_{i}">表示<img src="https://private.codecogs.com/gif.latex?H_%7Bi%7D" alt="H_{i}">中的弱分类器的数量，误报率（把一个多数类的样本分类成少数类）<img src="/S_%7Bmaj%7D%7D.gif" alt="f\Leftarrow \sqrt[T-1]{|S_{min}|/|S_{maj}|}">；</p>
</li>
<li><p>$i&#x3D;0$</p>
</li>
<li><p>从多数类<img src="/gif-16336577977311.latex" alt="img">中随机选择一个子集$E$,$|E|&#x3D;|S_{min}|$。</p>
</li>
<li><p>使用子集<img src="/gif-16336578232435.latex" alt="img">和<img src="/gif-16336578314437.latex" alt="img">，训练分类器$H_i$，$H_i$是由$s_i$个弱分类器$h_{i,j}$集合而成，其中每个弱分类器的权重为$\alpha_{i,j}$，阈值为：$\theta_i$。因此<br>$$<br>H_i(x)&#x3D;sgn(\sum_{j&#x3D;1}^{s_i}\alpha_{i,j}h_{i,j}(x)-\theta_i)<br>$$</p>
</li>
<li><p>重新计算误报率，据此调整$\theta_i$；</p>
</li>
<li><p>从$S_{maj}$中删除可以被$H_{i}$确分类的样本；</p>
</li>
<li><p>重复步骤 3-7，直到$i&#x3D;T$；</p>
</li>
<li><p>最后，输出一个集成学习器<br>$$<br>H(x)&#x3D;sgn(\sum_{i&#x3D;1}^{T}\sum_{j&#x3D;1}^{s_{i}}\alpha <em>{i,j}h</em>{i,j}(x)-\sum_{i&#x3D;1}^{T}\theta <em>{i})<br>$$<br>在这个方法中，每一次迭代过程都会造成数据集中多数类的数量减少，而且每一步的集成分类器都是由平衡的数据集训练得到的。只有当所有的$H</em>{i}(x)(i&#x3D;1,2,3,…,T)$预测为正例的时候，最终的分类器才会预测为正例。因此，这种方法在每次迭代过程中，在有限的样本空间利用尽可能多的信息。</p>
<p><strong>SMOTE(The Synthetic Minority Oversampling Technique):</strong></p>
<p>SMOTE是一种合成采样的一种解决不平衡学习的方法，它已经被证明在很多领域都比较有效。它主要是基于现存的少数类样本，计算样本特征空间之间的相似度，然后创建人工合成样本。</p>
<ol>
<li>对于少数类$S_{min}\in S$中的样本，即，$x_{i}\in S_{min}$，计算它的K个近邻；</li>
<li>通过计算 n 维空间的欧式距离，得到距离$x_{i}$最近的 K 个$S_{min}$中的样本数据；</li>
<li>然后从 K 个近邻中，随机选择一个样本，产生人工合成的数据；具体的方法如下：</li>
</ol>
<p>$x_{new}&#x3D;x_{i}+(\hat{x_{i}}-x_{i})\times \delta$，$x_{i}\in S_{min}$是一个少数类的样本，$\hat{x_{i}}$是$x_{i}$的其中一个近邻，$\delta \in [0,1]$是一个随机数。</p>
<p><img src="/image-20211008105604570.png" alt="image-20211008105604570"></p>
<p>上图展示了SMOTE的具体过程。（a）图展示了一个典型的不平衡的数据，SMOTE中的 K 取值为6。（b）图中展示了一个随机产生的合成样本，这个样本是沿着和的直线产生的。</p>
<p>SMOTE方法是一种过采样的方法，它克服了过采样的一些缺点，而且加强了原始数据。但是，SMOTE方法可能会造成一定的过拟合。</p>
<h4 id="代价敏感学习"><a href="#代价敏感学习" class="headerlink" title="代价敏感学习"></a>代价敏感学习</h4><p>代价敏感学习主要的是建立一个代价矩阵，代价矩阵中的数值代表样本被误分类之后的惩罚。例如，我们可以定义$C(Min,Maj)$为：把多数类的样本分类成少数类的代价，$C(Maj,Min)$表示把少数类的样本分类成多数类的代价。一般来说，把少数类的样本误分类所产生的代价往往比多熟练样本的误分类代价要大，因此，$C(Maj,Min)&gt;C(Min,Maj)$。代价敏感学习的目标是最小化整个训练集上的代价（贝叶斯条件风险）。</p>
<p>同时，在多分类任务中，使用$C(i,j),i,j \in {1,2,3,…,C}$表示把 <em>j</em> 中的样本误分类成 <em>i</em> 的代价。下图表示多分类任务中的一个代价矩阵。<br><img src="/image-20211008105626919.png" alt="image-20211008105626919"></p>
<p>在这个例子中，最终的条件风险为：$R(i|x)&#x3D;\sum _{j}P(j|x)C(i,j)$，其中$P(j|x)$表示样本被分类成 j 的概率。</p>
<p>主要有三种方法实现代价敏感学习：</p>
<ol>
<li>把误分类代价作为数据集的权重，然后采用 Bootstrap 采样方法选择具有最好的数据分布的数据集；</li>
<li>以集成学习的模式来实现代价最小化的技术，这种方法可以选择很多标准的学习算法作为集成学习中的弱分类器；</li>
<li>把代价敏感函数或者特征直接合并到分类器的参数中，这样可以更好的拟合代价敏感函数。由于这类技术往往都具有特定的参数，因此这类方法没有统一的框架；</li>
</ol>
<h4 id="数据生成-GAN"><a href="#数据生成-GAN" class="headerlink" title="数据生成(GAN)"></a>数据生成(GAN)</h4><p>GAN网络整体示意如下，这里的G网络的输入是一个符合简单分布如高斯分布或者均匀分布的随机噪声。</p>
<p><img src="/v2-48a6a2a8b213f4bd52dfb694ad292f00_720w.jpg" alt="img"></p>
<p> generator 生成的分布可以假设为 $PG(x;θ)$，这是一个由 θ 控制的分布，θ 是这个分布的参数（如果是高斯混合模型，那么 θ 就是每个高斯分布的平均值和方差）</p>
<p>在真实分布中取出一些数据，${x1, x2, … , x_m}$，我们想要计算一个似然 $PG(x_i; θ)$，生成模型的似然为<br>$$<br>L&#x3D;\prod_{i&#x3D;1}^mP_G(x^i;\theta)<br>$$<br>让generator生成的数据真实的概率最大，等价于让模型的似然最大，问题转变为最大似然估计问题，需要求最大似然参数$\theta^*$。<br>$$</p>
<p>$$<br><img src="/90.jpeg"></p>
</li>
</ol>
<p>GAN模型的目标函数如下</p>
<p><img src="/v2-64263acb7eeb012f7fa7e80446d4dac3_r.jpg" alt="preview"></p>
<p>GAN整体算法</p>
<p><img src="/v2-ce9f2c59b74e1970692fafe7d713fc99_r.jpg" alt="preview"></p>
<p><strong>3个优势</strong></p>
<ol>
<li>能更好建模数据分布（图像更锐利、清晰）</li>
<li>理论上，GANs 能训练任何一种生成器网络。其他的框架需要生成器网络有一些特定的函数形式，比如输出层是高斯的。</li>
<li>无需利用马尔科夫链反复采样，无需在学习过程中进行推断，没有复杂的变分下界，避开近似计算棘手的概率的难题。</li>
</ol>
<p><strong>2个缺陷</strong></p>
<ol>
<li>难训练，不稳定。生成器和判别器之间需要很好的同步，但是在实际训练中很容易D收敛，G发散。D&#x2F;G 的训练需要精心的设计。</li>
<li>模式缺失（Mode Collapse）问题。GANs的学习过程可能出现模式缺失，生成器开始退化，总是生成同样的样本点，无法继续学习。</li>
</ol>
<h3 id="特征增强-特征提取"><a href="#特征增强-特征提取" class="headerlink" title="特征增强(特征提取)"></a>特征增强(特征提取)</h3><p>数据的各项属性为异质，采用异质图谱构建样本特征。</p>
<p><strong>同质</strong>（Homogeneity ）与<strong>异质</strong>（Heterogeneity）系一对科学与统计学中常用的对一种生物组织或物质的均匀性进行描述的概念。如果一种材料或一幅图像具有同质性（或者说是同质的），那么它就是由同样的单元堆砌而成的，或者它各部分的特征（如颜色、形状、大小、重量、高度、分布、质地、语言、收入、疾病、温度、放射性、架构设计等）都是相同的。相对的，如果一种物质至少一种特征的分布明显不均匀，那么它就具有异质性。</p>
<p><strong>利用异质网络建模这种类型丰富且交互复杂的数据，可以保留更全面的语义及结构信息。</strong>相较于同质网络，异质网络建模带来了两方面的好处：</p>
<ol>
<li>异质网络不仅可以自然融合不同类型对象及其交互，而且可以融合异构数据源的信息。在大数据时代来源不同的数据仅捕获了部分甚至是有偏差的特征，异质图网络可以对这些数据进行综合处理。因此异质网络建模不仅成为解决大数据多样性的有力工具，而且成为宽度学习的主要方法。</li>
<li>异质网络包含丰富的结构和语义信息，为发现隐含模式提供了精准可解释的新途径。例如，推荐系统的异质网络中不再只有用户和商品这两种对象，而是包含店铺、品牌等更全面的内容，关系也不再只有购买，而是含有收藏、喜爱等更精细的交互。基于这些信息，利用元路径和元图等语义挖掘方法，可以产生更精细的知识发现。</li>
</ol>
<p><strong>异质图的相关概念</strong></p>
<p><img src="https://img-blog.csdnimg.cn/2020103111192626.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MDE1MDU5,size_16,color_FFFFFF,t_70#pic_center" alt="img"></p>
<p><img src="/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MDE1MDU5,size_16,color_FFFFFF,t_70#pic_center.png" alt="img"></p>
<p><strong>如何构建异质图注意力机制图神经网络</strong></p>
<ol>
<li>Heterogeneity of graph：如果确定图中的边，<strong>如何处理如此复杂的结构信息，同时保留多样的特征信息，是一个迫切需要解决的问题</strong></li>
<li>Semantic-level attention：<strong>如何为具体任务选择最有意义的元路径并融合语义信息是一个有待解决的问题</strong>。</li>
<li>Node-level attention：<strong>给定一个元路径，每个节点都有许多基于元路径的邻居。如何区分这些邻居之间的细微差别，选择具有信息的邻居是需要解决的问题。对于每个节点，节点级注意的目的是了解基于元路径的邻居的重要性，并为它们分配不同的注意值</strong>。</li>
</ol>
<p><strong>如何确定边-自适应近邻分类(the clustering with adaptive neighbor-CAN)</strong><br>$$<br>min\sum_{i,j&#x3D;1}^{n}||z_i-z_j||^2s_{ij}+\gamma <em>{ij}^2<br>\<br>s.t. s</em>{ij} \in (0,1), I^T_{s_i}&#x3D;1,rank(L_s)&#x3D;n-c<br>$$</p>
<p>其中，$z&#x3D;{z_1,z_2,…,z_n},z \in R^{n \times m}$，为数据样本集，$z_i$与$z_j$近邻可能性为$s_{ij}$，CAN的假设就是他们之间的距离越小，近邻可能性越大。$\gamma$为可调节参数，<em>I</em>为单位矩阵，$L_s$为拉普拉斯算子，由相似矩阵$s&#x3D;{s_1,s_2,…,s_n}$定义，图的连接关系可$L_s$的秩来给出。n为样本数量，c为连接的数量。</p>
<h3 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h3><h4 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h4><p>Boosting, 也称为增强学习或提升法，是一种重要的集成学习技术， 能够将预测精度仅比随机猜度略高的弱学习器增强为预测精度高的强学习器，这在直接构造强学习器非常困难的情况下，为学习算法的设计提供了一种有效的新思路和新方法。AdaBoost算法框架如下图所示</p>
<p><img src="/1.jpg" alt="1"></p>
<p><strong>Adaboost算法可以简述为三个步骤：</strong><br>（1）首先，是初始化训练数据的权值分布D1。假设有N个训练样本数据，则每一个训练样本最开始时，都被赋予相同的权值：w1&#x3D;1&#x2F;N。<br>（2）然后，训练弱分类器hi。具体训练过程中是：如果某个训练样本点，被弱分类器hi准确地分类，那么在构造下一个训练集中，它对应的权值要减小；相反，如果某个训练样本点被错误分类，那么它的权值就应该增大。权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。<br>（3）最后，将各个训练得到的弱分类器组合成一个强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。<br>换而言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。</p>
<p><strong>优点</strong></p>
<p>（1）Adaboost提供一种框架，在框架内可以使用各种方法构建子分类器。可以使用简单的弱分类器，不用对特征进行筛选，也<strong>不存在过拟合的现象</strong>。</p>
<p>（2）Adaboost算法不需要弱分类器的先验知识，最后得到的强分类器的分类精度依赖于所有弱分类器。无论是应用于人造数据还是真实数据，Adaboost都能显著的提高学习精度。</p>
<p>（3）Adaboost算法不需要预先知道弱分类器的错误率上限，且最后得到的强分类器的分类精度依赖于所有弱分类器的分类精度，可以深挖分类器的能力。Adaboost可以根据弱分类器的反馈，自适应地调整假定的错误率，执行的效率高。</p>
<p>（4）Adaboost对同一个训练样本集训练不同的弱分类器，按照一定的方法把这些弱分类器集合起来，构造一个分类能力很强的强分类器，即“三个臭皮匠赛过一个诸葛亮”。</p>
<p><strong>缺点：</strong></p>
<p>在Adaboost训练过程中，Adaboost会使得难于分类样本的权值呈指数增长，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰。此外，Adaboost依赖于弱分类器，而弱分类器的训练时间往往很长。</p>
<h4 id="GAT"><a href="#GAT" class="headerlink" title="GAT"></a>GAT</h4><p>GAT是一种典型的图神经网络方法， 注意力系数为表示为<br>$$<br>e_{ij}&#x3D;a(Wh_i,Wh_j)<br>$$<br>其中$a(.)$为线性函数，<em>W</em>为所有节点的权重系数矩阵，$e_{ij}$表示$V_i$与$V_j$的重要性，即节点之间的距离，为有效的将节点进行比对，采用softmax进行中心化，softmax为<br>$$<br>\alpha_{ij}&#x3D;soft \max(e_{ij})<br>$$<br>进一步的可以将节点$h_V^{·}$表示为<br>$$<br>h^._V&#x3D;g(x_V,E,\alpha_V)<br>$$<br>g(&#96;)是一个集合函数，则分类概率可表示为<br>$$<br>p(y_V|x_V,E)&#x3D;Cat(y_V|soft\max(Wh^._V))<br>$$<br>其中，Cat是一个类别分布，经过多次的更新后，获得一个合适标签类别分布。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol>
<li>数据样本不存在无标签情况，所有数据都是由汽车的下位机进行计算上传至服务器数据，分为故障与非故障，是否还能够用来半监督学习。</li>
<li>下位机已经计算得出了结果，服务器再计算一遍故障是否具有现实意义。</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://joyobama.github.io/2022/10/06/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%20-%20%E5%89%AF%E6%9C%AC/" data-id="cl8x3ovs4000cbgocaa4t0fzn" data-title="小样本学习" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/10/06/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Neuer</strong>
      <div class="article-nav-title">
        
          故障诊断方案
        
      </div>
    </a>
  
  
    <a href="/2022/10/06/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB2021%E5%B9%B49%E6%9C%8822%E6%97%A5/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Älter</strong>
      <div class="article-nav-title">锂电池电化学文献</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/10/">October 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/10/06/AD%E7%AE%80%E6%98%93%E6%95%99%E7%A8%8B/">AD简易教程</a>
          </li>
        
          <li>
            <a href="/2022/10/06/2022%E5%B9%B44%E6%9C%88FPGA%E5%BC%80%E5%8F%91%E7%9A%84%E4%B8%80%E4%BA%9B%E9%94%99%E8%AF%AF%E8%A6%81%E7%82%B9/">要点总结</a>
          </li>
        
          <li>
            <a href="/2022/10/06/2021%E5%B9%B4%E8%AE%B0%E8%B4%A6/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/10/06/%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E6%9D%BF%E8%A7%84%E5%88%92/">信号处理板计划</a>
          </li>
        
          <li>
            <a href="/2022/10/06/%E6%96%B0%E8%83%BD%E6%BA%90%E6%B1%BD%E8%BD%A6%E7%94%B5%E6%B1%A0%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/">新能源汽车电池数据清洗</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 Joy_Obama<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>